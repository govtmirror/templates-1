---
title: "Let's Talk About How To Ease Re-Analysis"
author: "Jake Bowers"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  beamer_presentation:
    keep_tex: yes
    latex_engine: xelatex
    template: sbst.beamer
    toc: yes
    slide_level: 2
  ioslides_presentation:
    logo: SBSTLightBulb.png
    smaller: yes
---
```{r setup, include=FALSE, echo=FALSE, cache=FALSE}
## the 'toc: yes' line above tricks pandoc into running latex twice so that we can see the nice logos

# Some customization.  You can alter or delete as desired (if you know what you are doing).
# knitr settings to control how R chunks work.

require(knitr)
opts_chunk$set(
  size = "scriptsize",    # slightly smaller font for code
  echo = TRUE,
  results = 'markup',
  strip.white = TRUE,
  cache = FALSE,
  highlight = TRUE,
  width.cutoff = 80,
  out.width = '.9\\textwidth',
  message = FALSE,
  comment = NA,
  tidy = TRUE,     # display code as typed
  tidy.opts = list(blank = FALSE, width.cutoff = 80))
```

# The Code Problem

## How can we have confidence in our results?

**If** Data analysis is computer programming.

**And** Computer programs have bugs.

**Then** How can anyone write code to do high stakes data analysis?


## General Strategies to Enhance Confidence and Reduce Bugs

1) Pairs Programming: If you and your colleagues are going to have to use each others' code, you may start writing code as if others were going to read it.

2) Unit Tests: Can we get the program to fail when it does something wrong?

3) Dependency Tracking: What file depends on what other file? Can we go from raw data to results in one command? Where did this figure come from?

4) Other ideas? 

# Nitty Gritty

## How to be nice to your future self and future friends with code?

```{r googlesheets1, size="scriptsize"}
## The first time this is run in a while, you'll need to run each line by hand. Thereafter, it should all work smoothly.
library(googlesheets)
countyCountsGS <- gs_title("countyCountsFIPS.csv") ## get info about the data
countyCounts <- as.data.frame(gs_read_csv(countyCountsGS,as.is=TRUE),stringsAsFactors=FALSE)
library(xtable)
xt1 <- xtable(with(countyCounts,tapply(n,state,mean)))
```

```{r xtab}
print(x1)
```

## General Data Sharing Ideas

Everyone start with the same original data and code to turn original data into clean working file, and code to turn working file into results (numbers, tables, figures)

If possible, keep data remote. If possible, keep data as plain text (.csv for example)

## Data Sharing Idea 1: Google Sheets

For example, some data can be stored in our Google Drive:

```{r googlesheets, size="scriptsize"}
## The first time this is run in a while, you'll need to run each line by hand. Thereafter, it should all work smoothly.
library(googlesheets)
countyCountsGS<-gs_title("countyCountsFIPS.csv") ## get info about the data
countyCounts<-as.data.frame(gs_read_csv(countyCountsGS,as.is=TRUE),stringsAsFactors=FALSE)
str(countyCounts)
```

## Data Sharing Idea 2: Data Online

Other data can be read directly from a website:
```{r}
usaCountiesPopCentroids<-read.csv(url("http://www2.census.gov/geo/docs/reference/cenpop2010/county/CenPop2010_Mean_CO.txt"),as.is=TRUE)
head(usaCountiesPopCentroids)
```

## Other Data Ideas

 - Small Text Files (.txt,.csv) can live on Github (currently this is all public)

## Data Ideas Specific to Experiments and the SBST

 - Experiments are defined by treatment assignments, fixed/pre-treatment design features (like blocks/strata), and outcomes. The analysis of experiments requires those three pieces of information for each unit.^[And we assume that, within block, the probability of treatment assignment is constant for each unit.]


# Best Practices Regarding Code


